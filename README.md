# Alumni Networking Tool

A web-based application designed to help the College of Engineering connect with students and alumni using LinkedIn data. The tool includes a powerful scraper to extract alumni information and provides an interface for exploring alumni profiles, networking opportunities, and fostering engagement between current students and graduates.

---

## Features

### LinkedIn Alumni Scraper
- **Automated Profile Scraping** â€” Selenium-based scraper with anti-bot defense layer
- **Groq AI Extraction** â€” LLM-powered extraction for experience and education data
- **Search Resume Checkpointing** â€” Search mode resumes from the last saved page (fresh for up to 7 days by default)
- **Multiple Scraping Modes:**
  - **Search Mode** â€” Iterates through UNT alumni search results
  - **Names Mode** â€” Searches specific names from a CSV file
  - **Connections Mode** â€” Scrapes from your LinkedIn connections
  - **Review Mode** â€” Re-scrapes flagged profiles; detects dead/removed URLs
  - **Update Mode** â€” Refreshes outdated profiles based on configurable frequency

### Data Extraction
- **Profile Information:** Name, headline, location
- **Work Experience:** Up to 3 jobs with company, title, and date ranges (Groq AI primary, CSS fallback)
- **Education:** School, degree/major, graduation year, school start date (Groq AI primary, CSS fallback)
- **Degree Normalization:** Deterministic mapping of raw degree strings to standardized forms (e.g. "B.S." â†’ "Bachelor of Science")
- **Working While Studying Detection:** Automatically determines if alumni worked during school
- **Smart Entity Classification:** Tiered system using database lookup, spaCy NER, and regex to accurately distinguish job titles from company names
- **Engineering Discipline Classification:** Smart categorization of alumni into 7 engineering disciplines based on Job Title, Degree, and Headline priority

### Data Management
- **CSV Output** â€” All scraped data saved to `scraper/output/UNT_Alumni_Data.csv`
- **MySQL Database** â€” Persistent storage with full profile data
- **SQLite Fallback** â€” Automatic local backup when cloud DB is unreachable
- **Visited Profile Tracking** â€” Prevents duplicate scraping across sessions
- **Flagged Profile Review** â€” Re-scrape specific profiles to fix data issues
- **Dead URL Detection** â€” Identifies removed/changed LinkedIn profiles during review mode
- **Profile Blocklist** â€” Permanently blocks fake/placeholder profiles from being scraped or saved
- **Smart Duplicate Handling** â€” New data overwrites old when re-scraped
- **CSV Data Cleanup** â€” Utility to fix swapped job titles/companies in existing data

### Web Application
- **Alumni Search** - Find alumni by name, graduation year, degree, or department
- **Profile Insights** - View LinkedIn profiles, career paths, and current positions
- **Interactive Dashboard** - Visualize alumni distribution by location, industry, and role
- **Alumni Location Heatmap** - Interactive map showing alumni distribution worldwide
- **Secure Data Storage** - MySQL database with geocoded coordinates
- **Access Control** - Restricted to UNT faculty/staff (unt.edu emails) with whitelist support

---

## Working While Studying Logic

`working_while_studying` is computed with this order:

1. Date-based logic first (existing behavior): compare graduation year/date context with job start/end context.
2. Missing-date fallback (strict): only used when date-based status is not computable.
3. Fallback returns `yes` only when all are true:
   - At least one UNT education entry exists (`University of North Texas` or `UNT`)
   - At least one experience normalizes to `Graduate Assistant`
   - That experience employer is UNT (`University of North Texas`, `UNT Libraries`, etc.; `HUNT` does not match)
4. Otherwise fallback returns `no`.

### Retroactive backfill for existing records

If you already have rows in the database from older logic, run:

```bash
python migrations/migrate_working_while_studying.py
```

This recomputes both:
- `alumni.working_while_studying_status`
- `alumni.working_while_studying`

---

## Engineering Discipline Classification

The system automatically classifies alumni into one of 7 categories.

**Priority Logic:** Job Title > Degree > Headline
(e.g., A "Lead Software Engineer" with a "Computer Engineering" degree is classified as "Software" because their current job is the source of truth.)

**Categories:**
1. Software, Data & AI Engineering
2. Embedded, Electrical & Hardware Engineering
3. Mechanical & Energy Engineering
4. Biomedical Engineering
5. Materials Science & Manufacturing
6. Construction & Engineering Management
7. Unknown

**Auto-Inference:**
New records imported via CSV are automatically classified. The system uses an ordered keyword matching algorithm to ensure accurate categorization (e.g., "Embedded Systems" takes precedence over generic "Systems").

**Retroactive Updates:**
To re-classify existing alumni records (e.g., after updating logic):
```bash
python backend/backfill_disciplines.py
```

---

## Quick Start

### Prerequisites
- Python 3.10+
- Google Chrome browser
- LinkedIn account
- MySQL database (for web app features)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/sangambartaula/alumni-networking-tool
   cd alumni-networking-tool
   ```

2. **Create a virtual environment:**
   ```bash
   python -m venv venv
   venv\Scripts\activate     # Windows
   source venv/bin/activate  # macOS/Linux
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables:**
   Create a `.env` file in the project root. See the [Setup Guide](SETUP_GUIDE.md) for a full template and instructions on getting API keys.

5. **Run the application:**
   ```bash
   python backend/app.py
   ```

For a complete walkthrough including LinkedIn API setup, see [SETUP_GUIDE.md](SETUP_GUIDE.md).

---

## LinkedIn Login Setup

For LinkedIn login to work, you must create your own LinkedIn App on the [LinkedIn Developer Portal](https://www.linkedin.com/developers/). This will generate a **Client ID** and **Client Secret**. Update the `.env` file with these credentials to enable authentication.

---

## Scraper Modes

### Search Mode
Iterates through LinkedIn's UNT alumni search results.
```
SCRAPER_MODE=search
```

Search mode automatically checkpoints the current results page in local SQLite state and resumes from that page on the next run when the checkpoint is still recent.

```
SCRAPE_RESUME_MAX_AGE_DAYS=7
```

If no results are found on a page, the checkpoint resets to page `1` for the next run.

### Names Mode
Searches for specific people from an input CSV file.
```
SCRAPER_MODE=names
INPUT_CSV=engineering_graduates.csv
```

### Connections Mode
Scrapes alumni from your LinkedIn connections.
```
SCRAPER_MODE=connections
CONNECTIONS_CSV=connections.csv
```

### Review Mode
Re-scrapes profiles listed in `scraper/output/flagged_for_review.txt`.
```
SCRAPER_MODE=review
```
Or: Add URLs to `flagged_for_review.txt` and the scraper will prompt you on startup.

**Dead URL Detection:** During review mode, the scraper detects when a LinkedIn profile returns "This page doesn't exist". At the end of the session, all dead URLs are listed and you're prompted to remove them from the database and history files:
```
============================================================
âš ï¸  3 DEAD / REMOVED PROFILES DETECTED:
============================================================
  ðŸ’€ https://linkedin.com/in/someone
  ðŸ’€ https://linkedin.com/in/anotherone
============================================================

Remove these profiles from database & history? [y/N]: y
âœ… Dead profiles cleaned from all data sources.
```

### Update Mode
Automatically detects profiles older than `UPDATE_FREQUENCY` and prompts to re-scrape.

---

## Flagging Profiles for Review

To fix bad data in existing profiles:

1. Add LinkedIn URLs to `scraper/output/flagged_for_review.txt` (one per line):
   ```
   https://www.linkedin.com/in/john-doe-123
   https://www.linkedin.com/in/jane-smith-456
   # This is a comment (ignored)
   ```

2. Run the scraper - it will detect flagged profiles and prompt you:
   ```
   ==================================================
   Found 2 profiles flagged for review.
   ==================================================
   >>> Run REVIEW mode to re-scrape them? (y/n): y
   ```

3. Successfully scraped profiles are removed from the file; failed ones remain for retry.

---

## Groq AI Extraction

The scraper uses [Groq](https://console.groq.com/) LLM to extract structured data from LinkedIn HTML, with CSS-based extraction as a fallback.

### How It Works
1. LinkedIn profile HTML is cleaned and structured
2. Sent to Groq (default model: `llama-3.1-8b-instant`, configurable via `GROQ_MODEL`)
3. Response is parsed as JSON and validated
4. Falls back to CSS selectors if Groq is unavailable or returns invalid data

### Modules
| Module | Purpose |
|--------|--------|
| `groq_client.py` | Shared client, API key handling, JSON parsing, debug HTML saving |
| `groq_extractor_experience.py` | Experience extraction (up to 3 jobs) |
| `groq_extractor_education.py` | Education extraction (degree, school, dates) |

### Configuration
```bash
# .env
GROQ_API_KEY=gsk_...         # Get from https://console.groq.com/keys
USE_GROQ=true                # Enable/disable Groq (falls back to CSS)
GROQ_MODEL=llama-3.1-8b-instant
SCRAPER_DEBUG_HTML=true      # Save raw HTML for debugging
```

---

## Degree Normalization

Raw degree strings from LinkedIn are normalized to standardized forms using a deterministic mapping.

| Raw Input | Normalized |
|-----------|------------|
| B.S. | Bachelor of Science |
| Master of Science in Computer Science | Master of Science |
| Ph.D. | Doctor of Philosophy |
| MBA | Master of Business Administration |

### Retroactive Migration
To normalize degrees for existing alumni records:
```bash
python migrations/migrate_normalize_degrees.py
```

---

## Profile Blocklist

Fake or placeholder LinkedIn profiles can be permanently blocked. Blocked profiles are:
- Skipped during scraping (all modes)
- Rejected by `save_profile_to_csv`
- Never saved to the database

To add a profile to the blocklist, add its LinkedIn slug to `BLOCKED_PROFILE_SLUGS` in `scraper/config.py`:
```python
BLOCKED_PROFILE_SLUGS = {
    "davidmartinez",
    "emilybrown",
    "johnsmith",
    # Add more slugs here...
}
```

---

## Automated Data Pipeline

After any scraping operation completes, the tool automatically:

1. **Syncs CSV to Database** â€” Imports/updates all profiles from `UNT_Alumni_Data.csv` to MySQL
2. **Updates Visited Profiles** â€” Ensures all alumni are tracked in the visited profiles table
3. **Geocodes New Locations** â€” Converts location strings to latitude/longitude for the heatmap

This automation runs in the `finally` block of `main.py`, so it happens even if you stop the scraper early.

### Manual Sync

If you need to run the sync manually (e.g., after importing data from another source):

```bash
cd backend

# Sync CSV to database (also runs migrations and stats)
python database.py

# Geocode missing locations
python geocoding.py
```

### CSV Data Cleanup

If you notice swapped job titles and company names in the CSV:

```bash
# Fix swapped entries and normalize text
python scraper/fix_csv_data.py
```

This script:
- Normalizes text (removes newlines, special characters)
- Auto-detects and fixes swapped job_title/company fields
- Applies known fixes for specific profiles
- Creates a backup before making changes
- **Interactive Data Validation**:
  ```bash
  # Check data against known companies/universities and train the classifier
  python scraper/check_data.py
  ```
  Scans `UNT_Alumni_Data.csv` and prompts you to verify new companies, universities, or job titles. This updates `scraper/data/companies.json` to improve future classification accuracy.

---

## Output Data

### CSV Columns
| Column | Description |
|--------|-------------|
| name | Full name |
| headline | LinkedIn headline |
| location | City, State, Country |
| job_title | Current/latest job title |
| company | Current/latest company |
| job_start_date | Job start date |
| job_end_date | Job end date |
| exp2_title, exp2_company, exp2_dates | Second experience |
| exp3_title, exp3_company, exp3_dates | Third experience |
| education | School name (UNT) |
| major | Degree/field of study |
| school_start_date | When they started school |
| graduation_year | Expected or actual graduation |
| working_while_studying | "yes", "currently", or "no" (stored status + boolean flag in DB) |
| profile_url | LinkedIn profile URL |
| scraped_at | Timestamp of scrape |

---

## Project Structure

```
alumni-networking-tool/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                    # Flask web application
â”‚   â”œâ”€â”€ database.py               # Database models and migrations
â”‚   â”œâ”€â”€ degree_normalization.py   # Deterministic degree normalization
â”‚   â”œâ”€â”€ job_title_normalization.py # Deterministic job title normalization
â”‚   â”œâ”€â”€ backfill_disciplines.py   # Engineering discipline classification
â”‚   â”œâ”€â”€ sqlite_fallback.py        # SQLite offline fallback system
â”‚   â”œâ”€â”€ geocoding.py              # Location geocoding service
â”‚   â””â”€â”€ alumni_backup.db          # Local SQLite backup (auto-generated)
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ alumni.html               # Alumni profile page
â”‚   â”œâ”€â”€ heatmap.html              # Alumni location heatmap
â”‚   â””â”€â”€ index.html                # Main landing page
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ main.py                   # Main scraper entry point
â”‚   â”œâ”€â”€ scraper.py                # LinkedIn scraping logic
â”‚   â”œâ”€â”€ config.py                 # Configuration, constants, and blocklist
â”‚   â”œâ”€â”€ groq_client.py            # Shared Groq LLM client infrastructure
â”‚   â”œâ”€â”€ groq_extractor_experience.py # AI experience extraction
â”‚   â”œâ”€â”€ groq_extractor_education.py  # AI education extraction
â”‚   â”œâ”€â”€ utils.py                  # Utility functions
â”‚   â”œâ”€â”€ database_handler.py       # CSV and history management
â”‚   â”œâ”€â”€ entity_classifier.py      # Job title/company classification
â”‚   â”œâ”€â”€ fix_csv_data.py           # CSV cleanup utility
â”‚   â”œâ”€â”€ defense/                  # Anti-bot defense layer
â”‚   â”‚   â”œâ”€â”€ navigator.py          # Safe navigation with health checks
â”‚   â”‚   â”œâ”€â”€ backoff.py            # Exponential backoff controller
â”‚   â”‚   â””â”€â”€ page_health.py        # Page health verification
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ companies.json        # Curated company/university database
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ UNT_Alumni_Data.csv        # Scraped data
â”‚       â”œâ”€â”€ flagged_for_review.txt     # Profiles to re-scrape
â”‚       â””â”€â”€ visited_history.csv        # Visited profile tracking
â”‚
â”œâ”€â”€ migrations/
â”‚   â”œâ”€â”€ migrate_normalize_titles.py   # Retroactive job title normalization
â”‚   â”œâ”€â”€ migrate_normalize_degrees.py  # Retroactive degree normalization
â”‚   â””â”€â”€ migrate_working_while_studying.py  # Retroactive working-while-studying recompute
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_degree_normalization.py  # Degree normalization tests
â”‚   â”œâ”€â”€ test_groq_imports.py          # Groq module import tests
â”‚   â””â”€â”€ test_scraper_logic.py         # Scraper logic tests
â”‚
â”œâ”€â”€ .env                    # Environment variables (not in git)
â”œâ”€â”€ .env.example            # Example environment file
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ SETUP_GUIDE.md          # Detailed setup tutorial
â””â”€â”€ GETTING_STARTED.md      # Getting started guide
```

---

## Testing

Run the test suite:
```bash
# Run full suite (root + backend tests)
./venv/bin/pytest -q

# Run entity classifier tests specifically
./venv/bin/pytest -q tests/test_entity_classifier.py
```

Tests cover:
- API route validation (including alumni filters)
- Database + SQLite fallback behavior
- Scraper data extraction logic
- Groq module imports and refactoring validation
- Degree normalization (exact matches, abbreviations, prefixes, edge cases)
- Entity classification (job titles, companies, locations, universities)
- Text normalization (newlines, special characters)
- Search resume state and dead URL cleanup safety
- Working-while-studying fallback and UNT matching rules

---

## Heatmap Feature

### Setup
```bash
# 1. Geocode all alumni locations
cd backend
python geocoding.py

# 2. Start the app
python app.py
# Open: http://localhost:5000/heatmap
```

### Features
- Interactive Leaflet map with zoom and pan
- Color-coded clustering (blue = low, red = high density)
- Clickable locations showing alumni count and sample profiles
- Real-time statistics

---

## SQLite Fallback (Offline Mode)

The application includes a local SQLite database backup that ensures the app continues to work when the cloud MySQL database is unreachableâ€”perfect for demos or network issues.

### How It Works

1. **On Startup:** Tries to connect to cloud MySQL
   - If reachable â†’ syncs a copy to local `alumni_backup.db`
   - If unreachable â†’ uses the existing local SQLite backup

2. **In Offline Mode:** 
   - All queries use the local SQLite database
   - Background thread silently retries cloud connection every 30 seconds
   - Any changes are recorded locally for later sync

3. **On Reconnection:**
   - Local changes are pushed to cloud (with smart merge)
   - Cloud updates are pulled to local
   - Conflicting changes â†’ cloud wins (source of truth)

### Testing SQLite Fallback

```bash
cd backend
python sqlite_fallback.py
```

This runs built-in tests and shows:
- Connection status (online/offline)
- Data sync status
- Table row counts
- Test results (WAL mode, table existence, etc.)

### Configuration

Add to `.env`:
```bash
# Enable SQLite fallback (default: enabled)
USE_SQLITE_FALLBACK=1

# Disable all DB operations (dev mode only)
DISABLE_DB=0
```

### API Endpoint

Check fallback status via API:
```
GET /api/fallback-status
```

Returns:
```json
{
  "success": true,
  "enabled": true,
  "is_offline": false,
  "last_cloud_sync": "2024-01-26T20:00:00+00:00",
  "pending_changes": 0,
  "discarded_changes": 0,
  "table_counts": {"alumni": 70, "users": 5, ...}
}
```

## Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| LINKEDIN_EMAIL | LinkedIn login email | Required |
| LINKEDIN_PASSWORD | LinkedIn login password | Required |
| SCRAPER_MODE | search, names, connections, review | names |
| SCRAPE_RESUME_MAX_AGE_DAYS | Max age (days) for search page checkpoint resume | 7 |
| UPDATE_FREQUENCY | How often to re-scrape profiles | 6 months |
| HEADLESS | Run Chrome without UI | false |
| TESTING | Enable shorter delays for testing | false |
| USE_COOKIES | Reuse cached LinkedIn cookies before credential login | false |
| INPUT_CSV | CSV file with names (names mode) | engineering_graduates.csv |
| CONNECTIONS_CSV | CSV of connections (connections mode) | connections.csv |
| GROQ_API_KEY | Groq API key for AI extraction | â€” |
| USE_GROQ | Enable Groq LLM extraction | true |
| GROQ_MODEL | Groq model used for LLM extraction/classification | llama-3.1-8b-instant |
| SCRAPER_DEBUG_HTML | Save raw HTML for debugging | false |
| USE_SQLITE_FALLBACK | Enable local SQLite backup | 1 (enabled) |
| DISABLE_DB | Disable all database operations | 0 (disabled) |
| FLAG_MISSING_GRAD_YEAR | Flag profile if grad year is missing | false |
| FLAG_MISSING_DEGREE | Flag profile if degree/major is missing | false |
| FLAG_MISSING_EXPERIENCE_DATA | Flag if job title/company is inconsistent | true |

---

## License

This project is for educational purposes. Use responsibly and in accordance with LinkedIn's Terms of Service.
